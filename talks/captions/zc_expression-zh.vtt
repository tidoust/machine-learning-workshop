WEBVTT

00:08.160 --> 00:08.940
幻灯片一

00:08.940 --> 00:11.660
首先我做一个简单的自我介绍

00:11.660 --> 00:15.900
我是来自网易的前端和客户端开发工程师陈泽伦

00:15.900 --> 00:17.460
今天我要讲的内容是

00:17.460 --> 00:22.660
一场基于机器学习加强表情表达的在线虚拟形象Web会议

00:22.660 --> 00:24.460
我们使用了WebAssembly

00:24.460 --> 00:26.940
将机器学习算法运行在浏览器上

00:26.940 --> 00:28.700
下面由我进行详细的演讲

00:30.820 --> 00:32.420
幻灯片二

00:32.420 --> 00:35.700
首先来讲述一下我们这个课题的背景

00:35.700 --> 00:41.899
由于疫情期间开设一场大型的会议由许多不方便的地方

00:41.899 --> 00:46.900
所以我们就想能不能在我们的游戏场景中搭建一个会议场景

00:46.900 --> 00:51.580
让演讲者和观众能够通过虚拟形象加入到会议中

00:51.580 --> 00:54.700
举办一个接近现实的沉浸式线上会议

00:54.700 --> 00:56.340
就如图中所示

00:56.340 --> 00:59.500
这是我们的会议场景 目前已经实现

00:59.500 --> 01:01.300
台下有观众的座位

01:01.300 --> 01:03.380
台上是演讲者的讲台

01:03.380 --> 01:06.100
演讲者可以在讲台上进行演讲

01:06.100 --> 01:09.580
并通过讲台中间的幻灯片幕布进行展示

01:09.580 --> 01:12.700
左下角是演讲者的虚拟形象

01:12.700 --> 01:16.420
目前观众看周围观众和讲师的面部表情都是僵硬的

01:16.420 --> 01:18.140
没有丰富的表情

01:18.140 --> 01:22.460
这就还没有达到我们沉浸式体验的目标

01:22.460 --> 01:25.100
下面我先简单介绍一下我们的架构

01:26.200 --> 01:28.780
幻灯片三

01:28.780 --> 01:30.740
我们的架构是这样的

01:30.740 --> 01:34.500
首先观众和演讲者通过浏览器打开Web页面

01:34.500 --> 01:38.420
然后通过WebRTC连接远端的游戏客户端

01:38.420 --> 01:44.100
之后可以通过本地的麦克风将声音通过WebRTC传递到游戏的声音模块

01:44.100 --> 01:47.580
实现演讲功能和答疑功能

01:47.580 --> 01:52.500
当我们完成会议的这么一个线上场景后 就像上面所说的

01:52.500 --> 01:56.900
我们想到一场成功的会议不仅需要这些会议的基本要素

01:56.900 --> 02:03.820
还需要加强参会者之间的互动来打造一个更贴近真实世界的沉浸式会议

02:03.820 --> 02:07.420
比如观众可以转头看到别的观众脸上的表情

02:07.420 --> 02:08.860
可以打招呼等

02:08.860 --> 02:11.381
观众们看到讲师脸上的表情

02:11.382 --> 02:14.460
我们经过一段时间的思考就想到

02:14.460 --> 02:21.579
我们可以 可能可以通过浏览器捕捉摄像头里的人脸转到虚拟人物模型上

02:21.579 --> 02:23.260
让大家看到

02:23.260 --> 02:28.900
正好我们算法组有一个基于机器学习的表情迁移的库

02:28.900 --> 02:31.380
我们就想着能不能将这个库跑在浏览器上呢

02:34.000 --> 02:35.260
幻灯片四

02:35.260 --> 02:40.420
这里先介绍一下我们整个算法工程的运行流程

02:40.420 --> 02:44.340
首先我们需要捕捉摄像头的内容 获取当前这一帧的画面

02:44.340 --> 02:52.340
将它发送给我们的算法工程在工程里去计算得到模型的顶点参数

02:52.340 --> 02:59.300
然后将这些顶点参数渲染在浏览器的的这个虚拟形象的模型上

02:59.300 --> 03:05.140
然后重复这整一个步骤 就实现了实时的将人脸数据转化为模型

03:05.140 --> 03:11.340
然后我们就想到 这个工程我们要跑到Web上应该怎么做呢

03:12.340 --> 03:14.980
幻灯片五

03:14.980 --> 03:19.740
由于我们的机器学习算法工程是基于C++实现的

03:19.740 --> 03:23.220
内部只依赖了Opencv库和一个机器学习推理框架

03:23.220 --> 03:27.780
那么我们所需要做的就是将这个C++工程运行在Web上

03:27.780 --> 03:29.260
于是我们想了三个方案

03:29.260 --> 03:35.260
第一个方案是将我们的C++工程代码重新用Javascript的代码去重写

03:35.260 --> 03:40.460
引用OpencvJs库和用JS写的机器学习推理框架的库

03:40.460 --> 03:44.140
但是这个方案需要同时掌握C++开发和JS开发

03:44.140 --> 03:47.620
并且需要需要知道两者实际上的差异

03:47.620 --> 03:49.740
这个的工作量是十分大的

03:49.740 --> 03:51.900
所以我们最后没有选择这个方案

03:51.980 --> 03:56.140
第二个方案是将算法工程部署在服务器上

03:56.140 --> 03:58.420
通过服务器的接口进行调用

03:58.420 --> 04:02.540
但是由于我们需要实时的将人脸数据转化为模型数据

04:02.540 --> 04:05.380
这种方式会有网络请求的耗时

04:05.380 --> 04:08.100
所以我们也没有选择这个方案

04:08.100 --> 04:13.180
第三个方案是将整个C++库编译成WebAssembly

04:13.180 --> 04:15.220
将它直接跑在浏览器上

04:15.220 --> 04:18.340
就没有上面那两个方案的那些问题

04:18.340 --> 04:21.140
最终我们选择了这第三个方案

04:23.200 --> 04:24.180
幻灯片六

04:24.980 --> 04:30.600
这就是一个常规的将C++库编译成WebAssembly的流程

04:30.600 --> 04:32.540
如图上这样所示

04:32.540 --> 04:37.200
我们打包成WASM文件后将其运行在浏览器上

04:37.200 --> 04:42.180
这里我们先给大家展示一下我们的成果

04:42.180 --> 04:45.700
首先是运行在浏览器上的效果

05:06.420 --> 05:11.180
大家可以看到 运行在浏览器上它的性能不是特别的好

05:11.180 --> 05:15.100
它大概需要两秒的时间将人脸数据转化为模型数据

05:15.100 --> 05:17.660
所以看起来有点卡顿

05:17.660 --> 05:20.220
接下来看一下我们在Unity上的效果

05:20.220 --> 05:23.600
这是我们所预期想要达到的效果

05:33.940 --> 05:34.420
幻灯片七

05:34.420 --> 05:37.620
通过上面两个视频的对比可以发现

05:37.620 --> 05:40.660
我们的工程跑在WebAssembly上性能不好

05:40.660 --> 05:47.580
我们在这里提出一些在整个算法工程中通过WebAssembly跑到Web上的过程中发现的一些问题

05:47.580 --> 05:48.820
和大家探讨一下

05:48.820 --> 05:51.420
首先 作为前端开发工程师

05:51.420 --> 05:54.500
我们打包这整个C++算法工程的过程中

05:54.500 --> 05:59.460
需要去了解CMake CLang 交叉编译和这些外部库的源码

05:59.460 --> 06:04.860
因为可能这些外部库的某些方法在WebAssembly上并不能跑

06:04.860 --> 06:06.980
我们在这里提出一个想法

06:06.980 --> 06:10.140
能不能构建一个类似于Npm的库环境

06:10.140 --> 06:15.620
让开发者编译后的LLVM编的Code库进行一个版本管理

06:15.620 --> 06:21.860
让无论懂不懂C++的WebAssembly开发者都能去简单地使用这些库

06:21.860 --> 06:26.740
第二个我们发现大部分的推理库都使用OpenMp

06:26.740 --> 06:28.620
并没有使用PThread

06:28.620 --> 06:33.180
那么我们就不能使用WebAssembly的多线程去进行优化

06:33.180 --> 06:38.980
除此之外 还有许多生态上的差异能否考虑去抹平

06:38.980 --> 06:45.200
第三个 我们在底层的矩阵计算时候发现它的性能十分的低下

06:45.200 --> 06:50.980
大家是否有一些这方面的方案供我们去优化

06:50.980 --> 06:59.660
第四个 我们的WASM文件跑在浏览器上
是可以通过逆向工程去反编译得到的源码结果

06:59.660 --> 07:03.460
那样对我们的算法的工程的保护性不是特别的好

07:03.460 --> 07:06.900
大家是否有一些可行性的方案

07:06.900 --> 07:10.380
以上四点是我们所遇到的问题

07:10.380 --> 07:17.980
然后我们也进行了一些自己的尝试对性能上面的一些优化

07:17.980 --> 07:23.460
比如 我们想到去使用一些效率较高的Javascript的库

07:23.460 --> 07:28.500
然后通过WebAssembly去通过EM_JS调用Javascript的方法

07:28.500 --> 07:35.260
当然这个方案并不是那么好 需要WASM和JS文件进行通信

07:35.260 --> 07:38.140
并且需要修改所有调用该库的地方

07:38.140 --> 07:43.580
能不能让Web经常使用的算法库写一些Web方法呢

07:43.580 --> 07:46.700
当调用环境是WebAssembly的时候

07:46.700 --> 07:49.600
可以自动地去底层调用ES_JM方法

07:49.600 --> 07:56.260
第二点 我们也尝试使用了SIMD的实验性方案

07:56.260 --> 07:58.140
在最新的浏览器上使用

07:58.140 --> 08:00.300
但是没有达到我们预期的要求

08:00.300 --> 08:04.200
首先是观众和演讲者浏览器未必是最新的

08:04.200 --> 08:08.540
其次 它对于我们的运算效率也没有显著的提升

08:11.860 --> 08:12.220
幻灯片八

08:12.220 --> 08:17.200
大家是否对我们的问题有一些看法或者观点呢

08:17.200 --> 08:18.100
欢迎告诉我们 谢谢


